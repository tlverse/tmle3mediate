<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Targeted Learning for Causal Mediation Analysis • tmle3mediate</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Targeted Learning for Causal Mediation Analysis">
<meta property="og:description" content="tmle3mediate">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-115145808-1"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-115145808-1');
</script>
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">tmle3mediate</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.0.3</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="https://tlverse.org">tlverse</a>
</li>
<li>
  <a href="../index.html">tmle3</a>
</li>
<li>
  <a href="../articles/intro_tmle3mediate.html">Overview</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/tlverse/tmle3mediate">
    <span class="fas fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Targeted Learning for Causal Mediation Analysis</h1>
                        <h4 class="author">
<a href="https://nimahejazi.org">Nima Hejazi</a>, <a href="https://statistics.berkeley.edu/people/james-duncan">James Duncan</a>, and <a href="http://bbd.berkeley.edu/cohort-4-2019-2020.html">David McCoy</a>
</h4>
            
            <h4 class="date">2021-04-18</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/tlverse/tmle3mediate/blob/master/vignettes/intro_tmle3mediate.Rmd"><code>vignettes/intro_tmle3mediate.Rmd</code></a></small>
      <div class="hidden name"><code>intro_tmle3mediate.Rmd</code></div>

    </div>

    
    
<div id="background" class="section level1">
<h1 class="hasAnchor">
<a href="#background" class="anchor"></a>Background</h1>
<p>A treatment often affects an outcome indirectly, through a particular pathway, by its effect on <em>intermediate variables</em> (mediators). Causal mediation analysis concerns the construction and evaluation of these <em>indirect effects</em> and the <em>direct effects</em> that are complementary to them. Generally, the indirect effect (IE) of a treatment on an outcome is the portion of the total effect that is found to work through mediators, while the direct effect includes all other components of the total effect (including both the effect of the treatment on the outcome and the effect through all paths not explicitly involving the mediators). Identifying and quantifying the mechanisms underlying causal effects is an increasingly popular endeavor in public health, medicine, and the social sciences, as such mechanistic knowledge improves understanding of both <em>why</em> and <em>how</em> treatments may be effective.</p>
<p>While the study of mediation analysis may be traced back quite far, the field only came into its modern form with the identification and careful study of the natural direct and indirect effects [<span class="citation">Robins and Greenland (1992)</span>; pearl2001direct]. The natural direct effect (NDE) and the natural indirect effect (NIE) are based on a decomposition of the average treatment effect (ATE) in the presence of mediators <span class="citation">(VanderWeele 2015)</span>, with requisite theory for the construction of efficient estimators of these quantities only receiving attention recently <span class="citation">(Tchetgen Tchetgen and Shpitser 2012)</span>. Here, we examine the use of the <code>tmle3mediate</code> package for constructing targeted maximum likelihood (TML) estimators of the NDE and NIE <span class="citation">(Zheng and van der Laan 2012)</span>.</p>
<p>To make our methodology and results comparable to that exposed in existing tools, we’ll take as a running example a simple dataset from an observational study of the relationship between BMI and kids’ behavior, distributed as part of the <a href="https://CRAN.R-project.org/package=mma"><code>mma</code> R package on CRAN</a>. First, let’s load the packages we’ll be using and set a seed; then, load this dataset and take a quick look at it</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># preliminaries</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org">dplyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyr.tidyverse.org">tidyr</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tlverse.org/sl3">sl3</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tlverse.org/tmle3">tmle3</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tlverse.org/tmle3mediate">tmle3mediate</a></span><span class="op">)</span>

<span class="co"># load and examine data</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.r-project.org">mma</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">weight_behavior</span><span class="op">)</span>

<span class="co"># set a seed</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">429153</span><span class="op">)</span></code></pre></div>
<p>The documentation for the dataset describes it as a “database obtained from the Louisiana State University Health Sciences Center, New Orleans, by Dr. Richard Scribner. He explored the relationship between BMI and kids’ behavior through a survey at children, teachers and parents in Grenada in 2014. This data set includes 691 observations and 15 variables.”</p>
<p>Unfortunately, the dataset contains a few observations with missing values. As these are unrelated to the demonstration of our analytic methodology, we’ll simply remove these for the time being. Note that in a real-world data analysis, we would instead consider strategies for working with the observed data and missing observations, including imputation and inverse probability of censoring weighting. For now, we simply remove the incomplete observations, resulting in a dataset with fewer observations but much the same structure as the original:</p>
<pre><code>## # A tibble: 567 x 15
##      bmi   age sex   race  numpeople   car gotosch snack tvhours cmpthours
##    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;     &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;dbl&gt;     &lt;dbl&gt;
##  1  18.2  12.2 F     OTHER         5     3 2       1           4         0
##  2  22.8  12.8 M     OTHER         4     3 2       1           4         2
##  3  25.6  12.1 M     OTHER         2     3 2       1           0         2
##  4  15.1  12.3 M     OTHER         4     1 2       1           2         1
##  5  23.0  11.8 M     OTHER         4     1 1       1           4         3
##  6  19.2  12.1 F     OTHER         3     3 2       1           0         0
##  7  16.6  12.4 M     OTHER         5     3 1       1           0         0
##  8  22.1  11.9 F     OTHER         5     2 2       1           3         4
##  9  15.9  12.4 F     OTHER         4     3 2       2           0         0
## 10  18.6  12.7 M     OTHER         5     3 2       1           0         0
## # … with 557 more rows, and 5 more variables: cellhours &lt;dbl&gt;, sports &lt;dbl&gt;,
## #   exercises &lt;int&gt;, sweat &lt;int&gt;, overweigh &lt;dbl&gt;</code></pre>
<p>For the analysis of this observational dataset, we focus on the effect of participating in a sports team (<code>sports</code>) on the BMI of children (<code>bmi</code>), taking several related covariates as mediators (<code>snack</code>, <code>exercises</code>, <code>overweigh</code>) and all other collected covariates as potential confounders. Considering an NPSEM, we separate the observed variables from the dataset into their corresponding nodes as follows:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">exposure</span> <span class="op">&lt;-</span> <span class="st">"sports"</span>
<span class="va">outcome</span> <span class="op">&lt;-</span> <span class="st">"bmi"</span>
<span class="va">mediators</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"snack"</span>, <span class="st">"exercises"</span>, <span class="st">"overweigh"</span><span class="op">)</span>
<span class="va">covars</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://generics.r-lib.org/reference/setops.html">setdiff</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">weight_behavior_complete</span><span class="op">)</span>,
                  <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">exposure</span>, <span class="va">outcome</span>, <span class="va">mediators</span><span class="op">)</span><span class="op">)</span>
<span class="va">node_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
  W <span class="op">=</span> <span class="va">covars</span>,
  A <span class="op">=</span> <span class="va">exposure</span>,
  Z <span class="op">=</span> <span class="va">mediators</span>,
  Y <span class="op">=</span> <span class="va">outcome</span>
<span class="op">)</span></code></pre></div>
<p>Here the <code>node_list</code> encodes the parents of each node – for example, <span class="math inline">\(Z\)</span> (the mediators) have parents <span class="math inline">\(A\)</span> (the treatment) and <span class="math inline">\(W\)</span> (the baseline confounders), and <span class="math inline">\(Y\)</span> (the outcome) has parents <span class="math inline">\(Z\)</span>, <span class="math inline">\(A\)</span>, and <span class="math inline">\(W\)</span>.</p>
<p>To use the natural (in)direct effects for mediation analysis, we decompose the ATE into its corresponding direct and indirect effect components. Throughout, we’ll make use of the (semiparametric efficient) TML estimators of <span class="citation">Zheng and van der Laan (2012)</span>, which allow for flexible ensemble machine learning to be incorporated into the estimation of nuisance parameters. For this, we rely on the <a href="https://tlverse.org/sl3"><code>sl3</code> R package</a> <span class="citation">(Coyle et al. 2020)</span>, which provides an implementation of machine learning pipelines and the Super Learner algorithm; for more on the <code>sl3</code> R package, consider consulting <a href="https://tlverse.org/tlverse-handbook/sl3.html">this chapter of the <code>tlverse</code> handbook</a>. Below, we construct an ensemble learner using a handful of popular machine learning algorithms:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># SL learners used for continuous data (the nuisance parameter M)</span>
<span class="va">enet_contin_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glmnet.html">Lrnr_glmnet</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  alpha <span class="op">=</span> <span class="fl">0.5</span>, family <span class="op">=</span> <span class="st">"gaussian"</span>, nfolds <span class="op">=</span> <span class="fl">3</span>
<span class="op">)</span>
<span class="va">lasso_contin_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glmnet.html">Lrnr_glmnet</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  alpha <span class="op">=</span> <span class="fl">1</span>, family <span class="op">=</span> <span class="st">"gaussian"</span>, nfolds <span class="op">=</span> <span class="fl">3</span>
<span class="op">)</span>
<span class="va">fglm_contin_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glm_fast.html">Lrnr_glm_fast</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">gaussian</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="va">mean_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_mean.html">Lrnr_mean</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span>
<span class="va">contin_learner_lib</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Stack.html">Stack</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  <span class="va">enet_contin_learner</span>, <span class="va">lasso_contin_learner</span>, <span class="va">fglm_contin_learner</span>, <span class="va">mean_learner</span>
<span class="op">)</span>
<span class="va">sl_contin_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_sl.html">Lrnr_sl</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>learners <span class="op">=</span> <span class="va">contin_learner_lib</span>,
                                 metalearner <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_nnls.html">Lrnr_nnls</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span><span class="op">)</span>

<span class="co"># SL learners used for binary data (nuisance parameters G and E in this case)</span>
<span class="va">enet_binary_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glmnet.html">Lrnr_glmnet</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  alpha <span class="op">=</span> <span class="fl">0.5</span>, family <span class="op">=</span> <span class="st">"binomial"</span>, nfolds <span class="op">=</span> <span class="fl">3</span>
<span class="op">)</span>
<span class="va">lasso_binary_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glmnet.html">Lrnr_glmnet</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  alpha <span class="op">=</span> <span class="fl">1</span>, family <span class="op">=</span> <span class="st">"binomial"</span>, nfolds <span class="op">=</span> <span class="fl">3</span>
<span class="op">)</span>
<span class="va">fglm_binary_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_glm_fast.html">Lrnr_glm_fast</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>family <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/family.html">binomial</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
<span class="va">binary_learner_lib</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Stack.html">Stack</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>
  <span class="va">enet_binary_learner</span>, <span class="va">lasso_binary_learner</span>, <span class="va">fglm_binary_learner</span>, <span class="va">mean_learner</span>
<span class="op">)</span>
<span class="va">logistic_metalearner</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://tlverse.org/sl3/reference/Lrnr_base.html">make_learner</a></span><span class="op">(</span><span class="va">Lrnr_solnp</span>,
                                     <span class="va">metalearner_logistic_binomial</span>,
                                     <span class="va">loss_loglik_binomial</span><span class="op">)</span>
<span class="va">sl_binary_learner</span> <span class="op">&lt;-</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_sl.html">Lrnr_sl</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>learners <span class="op">=</span> <span class="va">binary_learner_lib</span>,
                                 metalearner <span class="op">=</span> <span class="va">logistic_metalearner</span><span class="op">)</span>

<span class="co"># create list for treatment and outcome mechanism regressions</span>
<span class="va">learner_list</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>
  Y <span class="op">=</span> <span class="va">sl_contin_learner</span>,
  A <span class="op">=</span> <span class="va">sl_binary_learner</span>
<span class="op">)</span></code></pre></div>
</div>
<div id="the-natural-direct-and-indirect-effects" class="section level1">
<h1 class="hasAnchor">
<a href="#the-natural-direct-and-indirect-effects" class="anchor"></a>The Natural Direct and Indirect Effects</h1>
<div id="decomposing-the-average-treatment-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#decomposing-the-average-treatment-effect" class="anchor"></a>Decomposing the Average Treatment Effect</h2>
<p>The natural direct and indirect effects arise from a decomposition of the ATE: <span class="math display">\[\begin{equation*}
  \mathbb{E}[Y(1) - Y(0)] =
    \underbrace{\mathbb{E}[Y(1, Z(0)) - Y(0, Z(0))]}_{NDE} +
    \underbrace{\mathbb{E}[Y(1, Z(1)) - Y(1, Z(0))]}_{NIE}.
\end{equation*}\]</span> In particular, the natural indirect effect (NIE) measures the effect of the treatment <span class="math inline">\(A \in \{0, 1\}\)</span> on the outcome <span class="math inline">\(Y\)</span> through the mediators <span class="math inline">\(Z\)</span>, while the natural direct effect (NDE) measures the effect of the treatment on the outcome <em>through all other paths</em>.</p>
</div>
<div id="the-natural-direct-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#the-natural-direct-effect" class="anchor"></a>The Natural Direct Effect</h2>
<p>The NDE is defined as <span class="math display">\[\begin{equation*}
  \Psi_{NDE}=\mathbb{E}[Y(1, Z(0)) - Y(0, Z(0))]
  \overset{\text{rand.}}{=} \sum_w \sum_z
  [\underbrace{\mathbb{E}(Y \mid A = 1, z, w)}_{\bar{Q}_Y(A = 1, z, w)} -
  \underbrace{\mathbb{E}(Y \mid A = 0, z, w)}_{\bar{Q}_Y(A = 0, z, w)}] \times
  \underbrace{p(z \mid A = 0, w)}_{Q_Z(0, w))} \underbrace{p(w)}_{Q_W},
\end{equation*}\]</span> where the likelihood factors <span class="math inline">\(p(z \mid A = 0, w)\)</span> and <span class="math inline">\(p(w)\)</span> (among other conditional densities) arise from a factorization of the joint likelihood: <span class="math display">\[\begin{equation*}
  p(w, a, z, y) = \underbrace{p(y \mid w, a, z)}_{Q_Y(A, W, Z)}
  \underbrace{p(z \mid w, a)}_{Q_Z(Z \mid A, W)}
  \underbrace{p(a \mid w)}_{g(A \mid W)}
  \underbrace{p(w)}_{Q_W}.
\end{equation*}\]</span></p>
<p>The process of estimating the NDE begins by constructing <span class="math inline">\(\bar{Q}_{Y, n}\)</span>, an estimate of the outcome mechanism <span class="math inline">\(\bar{Q}_Y(Z, A, W) = \mathbb{E}[Y \mid Z, A, W]\)</span> (i.e., the conditional mean of <span class="math inline">\(Y\)</span>, given <span class="math inline">\(Z\)</span>, <span class="math inline">\(A\)</span>, and <span class="math inline">\(W\)</span>). With an estimate of this conditional expectation in hand, predictions of the counterfactual quantities <span class="math inline">\(\bar{Q}_Y(Z, 1, W)\)</span> (setting <span class="math inline">\(A = 1\)</span>) and, likewise, <span class="math inline">\(\bar{Q}_Y(Z, 0, W)\)</span> (setting <span class="math inline">\(A = 0\)</span>) can easily be obtained. We denote the difference of these counterfactual quantities <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span>, i.e., <span class="math inline">\(\bar{Q}_{\text{diff}} = \bar{Q}_Y(Z, 1, W) - \bar{Q}_Y(Z, 0, W)\)</span>. <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span> represents the difference in <span class="math inline">\(Y\)</span> attributable to changes in <span class="math inline">\(A\)</span> while keeping <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> at their <em>natural</em> (i.e. observed) values.</p>
<p>The estimation procedure treats <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span> itself as a nuisance parameter, regressing its estimate <span class="math inline">\(\bar{Q}_{\text{diff}, n}\)</span> on <span class="math inline">\(W\)</span>, among control observations only (i.e., those for whom <span class="math inline">\(A = 0\)</span> is observed); the goal of this step is to remove part of the marginal impact of <span class="math inline">\(Z\)</span> on <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span>, since <span class="math inline">\(W\)</span> is a parent of <span class="math inline">\(Z\)</span>. Regressing this difference on <span class="math inline">\(W\)</span> among the controls recovers the expected <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span>, had all individuals been set to the control condition <span class="math inline">\(A = 0\)</span>. Any residual additive effect of <span class="math inline">\(Z\)</span> on <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span> is removed during the TML estimation step using the auxiliary (or “clever”) covariate, which accounts for the mediators <span class="math inline">\(Z\)</span>. This auxiliary covariate take the form <span class="math display">\[\begin{equation*}
  C_Y(Q_Z, g)(O) = \Bigg\{\frac{\mathbb{I}(A = 1)}{g(1 \mid W)}
  \frac{Q_Z(Z \mid 0, W)}{Q_Z(Z \mid 1, W)} -
  \frac{\mathbb{I}(A = 0)}{g(0 \mid W)} \Bigg\}.
\end{equation*}\]</span> Breaking this down, <span class="math inline">\(\frac{\mathbb{I}(A = 1)}{g(1 \mid W)}\)</span> is the inverse probability weight for <span class="math inline">\(A = 1\)</span> and, likewise, <span class="math inline">\(\frac{\mathbb{I}(A = 0)}{g(0 \mid W)}\)</span> is the inverse probability weight for <span class="math inline">\(A = 0\)</span>. The middle term is the ratio of the mediator density when <span class="math inline">\(A = 0\)</span> to the mediator density when <span class="math inline">\(A = 1\)</span>.</p>
<p>Thus, it would appear that an estimate of this conditional density is required; unfortunately, tools to estimate such quantities are sparse in the statistics literature, and the problem is still more complicated (and computationally taxing) when <span class="math inline">\(Z\)</span> is high-dimensional. As only the ratio of these conditional densities is required, a convenient re-parametrization may be achieved, that is, <span class="math display">\[\begin{equation*}
  \frac{p(A = 0 \mid Z, W) g(0 \mid W)}{p(A = 1 \mid Z, W) g(1 \mid W)}.
\end{equation*}\]</span> Going forward, we will denote this re-parameterized conditional probability <span class="math inline">\(e(A \mid Z, W) := p(A \mid Z, W)\)</span>. This is particularly useful since the problem is reduced to the estimation of conditional means, opening the door to the use of a wide range of machine learning algorithms (e.g., most of those in <a href="https://github.com/tlverse/sl3"><code>sl3</code></a>).</p>
<p>Underneath the hood, the counterfactual outcome difference <span class="math inline">\(\bar{Q}_{\text{diff}}\)</span> and <span class="math inline">\(e(A \mid Z, W)\)</span>, the conditional probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span>, are used in constructing the auxiliary covariate for TML estimation. These nuisance parameters play an important role in the bias-correcting <em>TMLE-update step</em>.</p>
</div>
<div id="the-natural-indirect-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#the-natural-indirect-effect" class="anchor"></a>The Natural Indirect Effect</h2>
<p>Derivation and estimation of the NIE is analogous to that of the NDE. The NIE is the effect of <span class="math inline">\(A\)</span> on <span class="math inline">\(Y\)</span> <em>only through the mediator(s) <span class="math inline">\(Z\)</span></em>. This quantity – known as the (additive) natural indirect effect <span class="math inline">\(\mathbb{E}(Y(Z(1), 1) - \mathbb{{E}(Y(Z(0), 1)\)</span> – corresponds to the difference of the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(A = 1\)</span> and <span class="math inline">\(Z(1)\)</span> (the values the mediator would take under <span class="math inline">\(A = 1\)</span>) and the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(A = 1\)</span> and <span class="math inline">\(Z(0)\)</span> (the values the mediator would take under <span class="math inline">\(A = 0\)</span>).</p>
<p>As with the NDE, the re-parameterization trick can be used to estimate <span class="math inline">\(\mathbb{E}(A \mid Z, W)\)</span>, avoiding estimation of a possibly multivariate conditional density. However, in this case, the mediated mean outcome difference, denoted <span class="math inline">\(\Psi_Z(Q)\)</span>, is instead estimated as follows <span class="math display">\[\begin{equation*}
  \Psi_{NIE}(Q) = \mathbb{E}_{QZ}(\Psi_{NIE, Z}(Q)(1, W) -
    \Psi_{NIE, Z}(Q)(0, W))
\end{equation*}\]</span></p>
<p>Phrased plainly, this uses <span class="math inline">\(\bar{Q}_Y(Z, 1, W)\)</span> (the predicted values for <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z\)</span> and <span class="math inline">\(W\)</span> when <span class="math inline">\(A = 1\)</span>) and regresses this vector on <span class="math inline">\(W\)</span> among the treated units (for whom <span class="math inline">\(A = 1\)</span> is observed) to obtain the conditional mean <span class="math inline">\(\Psi_{NIE, Z}(Q)(1, W)\)</span>. Performing the same procedure, but now regressing <span class="math inline">\(\bar{Q}_Y(Z, 1, W)\)</span> on <span class="math inline">\(W\)</span> among the control units (for whom <span class="math inline">\(A = 0\)</span> is observed) yields <span class="math inline">\(\Psi_{NIE,Z}(Q)(0, W)\)</span>. The difference of these two estimates is the NIE and can be thought of as the additive marginal effect of treatment on the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(W\)</span>, <span class="math inline">\(A = 1\)</span>, <span class="math inline">\(Z\)</span> through its effects on <span class="math inline">\(Z\)</span>. So, in the case of the NIE, our estimate <span class="math inline">\(\psi_n\)</span> is slightly different, but the same quantity <span class="math inline">\(e(A \mid Z, W)\)</span> comes into play as the auxiliary covariate.</p>
<!--
## Targeted Maximum Likelihood

With each nuisance parameter estimated via flexible machine learning, _targeted
maximum likelihood_ is necessary to obtain targeted (i.e., de-biased) fits for
both the NDE and NIE estimates.

With targeted maximum likelihood estimation (TMLE), the portion of $P_0$ that
the target parameter is a function of is estimated, this is done by utilizing
additional components of $P_0$ outside of only $Q_y$ -- which, as seen above is
the only component to be used so far (for our $\Psi$ estimates). As $Q_Y$ is a
more complicated estimate the variance and bias is "spread" across our
parameter space, TMLE is an additional step to help target our parameter of
interest and reduce bias/variance for this parameter. The steps to TMLE for a
standard average treatment effect are:

1. Estimate $Q_Y$ using flexible machine learning,
2. Get predictions from the model for A = a and W = w, this is the initial
   prediction values for Y $\bar{Q}^0_n(A, W) = \mathbb{E}[Y \mid A, W]$,
3. We then use clever covariates (like the ones described above) in a
   parametric sub-model through the initial fit of $\bar{Q}^0_n(A,W)$, and
   estimate the unknown parameter of this submodel (the coefficients) which
   represents the amount of fluctuation of the initial fit (amount of residual
   confounding).

This is done by constructing a logistic regression flucuation model which has
the observed $Y$ as outcome, offset by the initial estimates $\var{Q}^0_n(A,W)$
and the auxiliary covariate as independent variable. Once fit, the coefficient
is used to update the initial predictions $\bar{Q}^0_n(A,W)$ to make
$\bar{Q}^1_n(A,W)$, if convergence to zero is not attained then this estimate
$\bar{Q}^1_n(A,W)$ can be used in the next iteration of the logistic regression
as an offset with Y as outcome and the clever coefficient to attain
$\bar{Q}^2_n(A,W)$. Usually, convergence occurrs in a single step, though this
is not always the case for more complex estimators. At each step, the initial
counterfactual estimates from the model are updated by:
$$logit\bar{Q}^1_{n}(A,W) = logit\bar{Q}^0_n(A,W) +
  \epsilon_n H_n^{\star}(A,W)$$
$$logit\bar{Q}^1_{n}(1,W) = logit\bar{Q}^0_n(1,W) + \epsilon_n
  H_n^{\star}(1,W)$$
$$logit\bar{Q}^1_{n}(0,W) = logit\bar{Q}^0_n(0,W) + \epsilon_n
  H_n^{\star}(0,W)$$,
for all observations

Until convergence ($\epsilon \approx 0$). Here, $\bar{Q}^0_n(1,W)$ and
$\bar{Q}^0_n(0,W)$ are the initial counterfactual outcome estimates (before
updating), $H_n^{\star}(1,W)$ is the clever covariate for treated observations
($A=1$) and $H_n^{\star}(0,W)$ is the clever covariate for control cases
($A=0$) and $\epsilon$ is the coefficient from the logistic regression model.

For the NIE and NDE, an analogous procedure is pursued; however, our updated
estimates, $logit\bar{Q}^{\star}_{n}(A,W)$, $logit\bar{Q}^{\star}_{n}(1,W)$,
and $logit\bar{Q}^{\star}_{n}(0,W)$ "backpropogate" to our $\Psi_{NDE}$ and
$\Psi_{NIE}$ estimates. So, for example, after the first TMLE fit, the
difference between $logit\bar{Q}^1_{n}(1,W)$ and $logit\bar{Q}^1_{n}(0,W)$ now
become $\bar{Q}_{\text{diff}}$, which is then regressed on W where $A = 0$ to
obtain our updated $\Psi_{NDE}$. Likewise, $logit\bar{Q}^1_{n}(1,W)$ is passed
back to $\Psi_{NIE}$ and used as the outcome when regressing on W when $A = 1$,
and $W$ when $A = 0$, as described above. Note, here we use the term regressing
loosely, meaning estimating using flexible machine learning algorithms.
-->
</div>
<div id="estimating-the-natural-indirect-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#estimating-the-natural-indirect-effect" class="anchor"></a>Estimating the Natural Indirect Effect</h2>
<p>We demonstrate calculation of the NIE using <code>tmle3mediate</code> below, starting by instantiating a “Spec” object that encodes exactly which learners to use for the nuisance parameters <span class="math inline">\(e(A \mid Z, W)\)</span> and <span class="math inline">\(\Psi_Z\)</span>. We then pass our Spec object to the <code>tmle3</code> function, alongside the data, the node list (created above), and a learner list indicating which machine learning algorithms to use for estimating the nuisance parameters based on <span class="math inline">\(A\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmle_spec_NIE</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tmle_NIE.html">tmle_NIE</a></span><span class="op">(</span>
  e_learners <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_cv.html">Lrnr_cv</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lasso_binary_learner</span>, full_fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
  psi_Z_learners <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_cv.html">Lrnr_cv</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lasso_contin_learner</span>, full_fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
  max_iter <span class="op">=</span> <span class="fl">1</span>
<span class="op">)</span>
<span class="va">weight_behavior_NIE</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://tlverse.org/tmle3/reference/tmle3.html">tmle3</a></span><span class="op">(</span>
  <span class="va">tmle_spec_NIE</span>, <span class="va">weight_behavior_complete</span>, <span class="va">node_list</span>, <span class="va">learner_list</span>
<span class="op">)</span>
<span class="va">weight_behavior_NIE</span></code></pre></div>
<pre><code>## A tmle3_Fit that took 1 step(s)
##    type                  param init_est tmle_est        se      lower    upper
## 1:  NIE NIE[Y_{A=1} - Y_{A=0}] 1.006857 1.009772 0.5130378 0.00423685 2.015308
##    psi_transformed lower_transformed upper_transformed
## 1:        1.009772        0.00423685          2.015308</code></pre>
</div>
<div id="estimating-the-natural-direct-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#estimating-the-natural-direct-effect" class="anchor"></a>Estimating the Natural Direct Effect</h2>
<p>An analogous procedure applies for estimation of the NDE, only replacing the Spec object for the NIE with <code>tmle_spec_NDE</code> to define learners for the NDE nuisance parameters:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmle_spec_NDE</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tmle_NDE.html">tmle_NDE</a></span><span class="op">(</span>
  e_learners <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_cv.html">Lrnr_cv</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lasso_binary_learner</span>, full_fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
  psi_Z_learners <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_cv.html">Lrnr_cv</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lasso_contin_learner</span>, full_fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
  max_iter <span class="op">=</span> <span class="fl">1</span>
<span class="op">)</span>
<span class="va">weight_behavior_NDE</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://tlverse.org/tmle3/reference/tmle3.html">tmle3</a></span><span class="op">(</span>
  <span class="va">tmle_spec_NDE</span>, <span class="va">weight_behavior_complete</span>, <span class="va">node_list</span>, <span class="va">learner_list</span>
<span class="op">)</span>
<span class="va">weight_behavior_NDE</span></code></pre></div>
<pre><code>## A tmle3_Fit that took 1 step(s)
##    type                  param   init_est   tmle_est        se     lower
## 1:  NDE NDE[Y_{A=1} - Y_{A=0}] 0.01828094 0.01828094 0.6911789 -1.336405
##       upper psi_transformed lower_transformed upper_transformed
## 1: 1.372967      0.01828094         -1.336405          1.372967</code></pre>
<!--
Let's check out what MMA looks like for this sports behavior -> BMI dataset as
mediated by these same three covariates:


```r
x <- weight_behavior_complete[, c(2:11, 13:15)]
pred <- weight_behavior_complete[, 12]
y <- data.frame(weight_behavior_complete[, 1])
colnames(y) <- "bmi"

mma_glm_weight <- mma(x, y, pred = pred, mediator = c(7, 11, 13),
                      jointm = list(n = 1, j1 = c(7, 11, 13)), predref = 0,
                      alpha = 0.4, alpha2 = 0.4, n2 = 20, nonlinear = FALSE)
print(mma_glm_weight)

mma_mart_weight <- mma(x, y, pred = pred, mediator = c(7, 11, 13),
                       jointm = list(n = 1, j1 = c(7, 11, 13)), predref = 0,
                       alpha = 0.4, alpha2 = 0.4, n = 2, n2 = 1,
                       nonlinear = TRUE)
print(mma_mart_weight)
```
-->
<!--
Now let's generate some simulated data where we know the truth and compare our
results using `tmle3mediate` to the `mma` package where we got the previous
data from:

```r
data_sim <- sim_data(n_obs = 2000)
head(data_sim)
```


```r
node_list <- list(
  W = c("W1", "W2", "W3"),
  A = "A",
  Z = c("Z1", "Z2", "Z3"),
  Y = "Y"
)

learner_list <- list(
  Y = cv_hal_contin_lrnr,
  A = cv_hal_binary_lrnr
)
tmle_spec_NIE <- tmle_NIE(
  e_learners = cv_hal_binary_lrnr,
  psi_Z_learners = cv_hal_contin_lrnr,
  max_iter = 10
)
NIE_est <- tmle3(tmle_spec_NIE, data_sim, node_list, learner_list)

tmle_spec_NDE <- tmle_NDE(
  e_learners = cv_hal_binary_lrnr,
  psi_Z_learners = cv_hal_contin_lrnr,
  max_iter = 10
)
NDE_est <- tmle3(tmle_spec_NDE, data_sim, node_list, learner_list)
```

Let's use mma to find the NDE and NIE on the same simulated data:

```r
x <- data.frame(data_sim[, c(1:3, 5:7)])
pred <- data.frame(data_sim[, 4])
y <- data.frame(data_sim[, 8])
colnames(y) <- "Y"

mma_glm <- mma(x, y, pred = pred, mediator = c(4:6),
               jointm = list(n = 1, j1 = c(4:6)), predref = 0,
               alpha = 0.4, alpha2 = 0.4, n2 = 20, nonlinear = FALSE)
print(mma_glm)

mma_mart <- mma(x, y, pred = pred, mediator = c(4:6),
                jointm = list(n = 1, j1 = c(4:6)), predref = 0,
                alpha = 0.4, alpha2 = 0.4, n2 = 1, nonlinear = TRUE)
print(mma_glm)
```
-->
</div>
</div>
<div id="the-population-intervention-direct-and-indirect-effects" class="section level1">
<h1 class="hasAnchor">
<a href="#the-population-intervention-direct-and-indirect-effects" class="anchor"></a>The Population Intervention Direct and Indirect Effects</h1>
<p>At times, the natural direct and indirect effects may prove too limiting, as these effect definitions are based on <em>static interventions</em> (i.e., setting <span class="math inline">\(A = 0\)</span> or <span class="math inline">\(A = 1\)</span>), which may be unrealistic for real-world interventions. In such cases, one may turn instead to the population intervention direct effect (PIDE) and the population intervention indirect effect (PIIE), which are based on decomposing the effect of the population intervention effect (PIE) of flexible stochastic interventions <span class="citation">(Dı́az and Hejazi 2020)</span>.</p>
<p>A particular type of stochastic intervention well-suited to working with binary treatments is the <em>incremental propensity score intervention</em> (IPSI), first proposed by <span class="citation">(<span class="citeproc-not-found" data-reference-id="kennedy2017nonparametric"><strong>???</strong></span>)</span>. Such interventions do not deterministically set the treatment level of an observed unit to a fixed quantity (i.e., setting <span class="math inline">\(A = 1\)</span>), but instead <em>alter the odds of receiving the treatment</em> by a fixed amount (<span class="math inline">\(0 \leq \delta \leq \infty\)</span>) for each individual. In the case of the <code>mma</code> dataset, we will proceed by considering an IPSI that modulates the odds of participating in a sports team by <span class="math inline">\(\delta = 2\)</span>. Such an intervention may be interpreted (hypothetically) as the effect of a school program that motivates children to participate in sports teams (e.g., as in an encouragement design). To exemplify our approach, we postulate a motivational intervention that <em>doubles the odds</em> (i.e., <span class="math inline">\(\delta = 2\)</span>) of participating in a sports team for each individual:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">delta_ipsi</span> <span class="op">&lt;-</span> <span class="fl">2</span></code></pre></div>
<div id="decomposing-the-population-intervention-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#decomposing-the-population-intervention-effect" class="anchor"></a>Decomposing the Population Intervention Effect</h2>
<p>We may decompose the population intervention effect (PIE) in terms of the <em>population intervention direct effect</em> (PIDE) and the <em>population intervention indirect effect</em> (PIIE): <span class="math display">\[\begin{equation*}
  \mathbb{E}\{Y(A_\delta)\} - \mathbb{E}Y =
    \overbrace{\mathbb{E}\{Y(A_\delta, Z(A_\delta))
      - Y(A_\delta, Z)\}}^{\text{PIIE}} +
    \overbrace{\mathbb{E}\{Y(A_\delta, Z) - Y(A, Z)\}}^{\text{PIDE}}.
\end{equation*}\]</span></p>
<p>This decomposition of the PIE as the sum of the population intervention direct and indirect effects has an interpretation analogous to the corresponding standard decomposition of the average treatment effect. In the sequel, we will compute each of the components of the direct and indirect effects above using appropriate estimators as follows</p>
<ul>
<li>For <span class="math inline">\(\mathbb{E}\{Y(A, Z)\}\)</span>, the sample mean <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n Y_i\)</span> is consistent;</li>
<li>for <span class="math inline">\(\mathbb{E}\{Y(A_{\delta}, Z)\}\)</span>, a TML estimator for the effect of a joint intervention altering the treatment mechanism but not the mediation mechanism, based on the proposal in <span class="citation">Dı́az and Hejazi (2020)</span>; and,</li>
<li>for <span class="math inline">\(\mathbb{E}\{Y(A_{\delta}, Z_{A_{\delta}})\}\)</span>, an efficient estimator for the effect of a joint intervention altering both the treatment and mediation mechanisms, as proposed in <span class="citation">(<span class="citeproc-not-found" data-reference-id="kennedy2017nonparametric"><strong>???</strong></span>)</span> and implemented in the <a href="https://github.com/ehkennedy/npcausal"><code>npcausal</code> R package</a>.</li>
</ul>
</div>
<div id="estimating-the-effect-decomposition-term" class="section level2">
<h2 class="hasAnchor">
<a href="#estimating-the-effect-decomposition-term" class="anchor"></a>Estimating the Effect Decomposition Term</h2>
<p>As described by <span class="citation">Dı́az and Hejazi (2020)</span>, the statistical functional identifying the decomposition term that appears in both the PIDE and PIIE <span class="math inline">\(\mathbb{E}\{Y(A_{\delta}, Z)\}\)</span>, which corresponds to altering the treatment mechanism while keeping the mediation mechanism fixed, is <span class="math display">\[\begin{equation*}
  \theta_0(\delta) = \int m_0(a, z, w) g_{0,\delta}(a \mid w) p_0(z, w)
    d\nu(a, z, w),
\end{equation*}\]</span> for which a TML estimator is available. The corresponding <em>efficient influence function</em> (EIF) with respect to the nonparametric model <span class="math inline">\(\mathcal{M}\)</span> is <span class="math inline">\(D_{\eta,\delta}(o) = D^Y_{\eta,\delta}(o) + D^A_{\eta,\delta}(o) + D^{Z,W}_{\eta,\delta}(o) - \theta(\delta)\)</span>.</p>
<p>The TML estimator may be computed basd on the EIF estimating equation and may incorporate cross-validation <span class="citation">(Zheng and van der Laan 2011; Chernozhukov et al. 2018)</span> to circumvent possibly restrictive entropy conditions (e.g., Donsker class). The resultant estimator is <span class="math display">\[\begin{equation*}
  \hat{\theta}(\delta) = \frac{1}{n} \sum_{i = 1}^n D_{\hat{\eta}_{j(i)},
  \delta}(O_i) = \frac{1}{n} \sum_{i = 1}^n \left\{ D^Y_{\hat{\eta}_{j(i)},
  \delta}(O_i) + D^A_{\hat{\eta}_{j(i)}, \delta}(O_i) +
  D^{Z,W}_{\hat{\eta}_{j(i)}, \delta}(O_i) \right\},
\end{equation*}\]</span> which is implemented in <code>tmle3mediate</code> (a one-step estimator is also avaialble, in the <a href="https://github.com/nhejazi/medshift"><code>medshift</code> R package</a>). We demonstrate the use of <code>tmle3mediate</code> to obtain <span class="math inline">\(\mathbb{E}\{Y(A_{\delta}, Z)\}\)</span> via its TML estimator:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># instantiate tmle3 spec for stochastic mediation</span>
<span class="va">tmle_spec_pie_decomp</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tmle_medshift.html">tmle_medshift</a></span><span class="op">(</span>
  delta <span class="op">=</span> <span class="va">delta_ipsi</span>,
  e_learners <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_cv.html">Lrnr_cv</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lasso_binary_learner</span>, full_fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,
  phi_learners <span class="op">=</span> <span class="va"><a href="https://tlverse.org/sl3/reference/Lrnr_cv.html">Lrnr_cv</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">lasso_contin_learner</span>, full_fit <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>
<span class="op">)</span>

<span class="co"># compute the TML estimate</span>
<span class="va">weight_behavior_pie_decomp</span> <span class="op">&lt;-</span> <span class="fu"><a href="http://tlverse.org/tmle3/reference/tmle3.html">tmle3</a></span><span class="op">(</span>
  <span class="va">tmle_spec_pie_decomp</span>, <span class="va">weight_behavior_complete</span>, <span class="va">node_list</span>, <span class="va">learner_list</span>
<span class="op">)</span>
<span class="va">weight_behavior_pie_decomp</span></code></pre></div>
<pre><code>## A tmle3_Fit that took 1672 step(s)
##    type         param init_est tmle_est        se    lower    upper
## 1: PIDE E[Y_{A=NULL}] 19.15858 19.14498 0.1872736 18.77793 19.51203
##    psi_transformed lower_transformed upper_transformed
## 1:        19.14498          18.77793          19.51203</code></pre>
</div>
<div id="estimating-the-population-intervention-direct-effect" class="section level2">
<h2 class="hasAnchor">
<a href="#estimating-the-population-intervention-direct-effect" class="anchor"></a>Estimating the Population Intervention Direct Effect</h2>
<p>Recall that, based on the decomposition outlined previously, the population intervention direct effect may be denoted <span class="math inline">\(\beta_{\text{PIDE}}(\delta) = \theta_0(\delta) - \mathbb{E}Y\)</span>. Thus, an estimator of the PIDE, <span class="math inline">\(\hat{\beta}_{\text{PIDE}}(\delta)\)</span> may be expressed as a composition of estimators of its constituent parameters: <span class="math display">\[\begin{equation*}
  \hat{\beta}_{\text{PIDE}}({\delta}) = \hat{\theta}(\delta) -
  \frac{1}{n} \sum_{i = 1}^n Y_i.
\end{equation*}\]</span></p>
<p>Based on the above, we may construct an estimator of the PIDE using quantities already computed. To do this, we need only apply the delta method, available from the <a href="https://github.com/tlverse/tmle3"><code>tmle3</code> package</a>.</p>
<!--

```r
tmle_task <- tmle_spec_pie_decomp$make_tmle_task(
  weight_behavior_complete, node_list
)
initial_likelihood <- tmle_spec_pie_decomp$make_initial_likelihood(
  tmle_task, learner_list
)
```
-->
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-chernozhukov2018double">
<p>Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/Debiased Machine Learning for Treatment and Structural Parameters.” <em>The Econometrics Journal</em> 21 (1). <a href="https://doi.org/10.1111/ectj.12097">https://doi.org/10.1111/ectj.12097</a>.</p>
</div>
<div id="ref-coyle2020sl3">
<p>Coyle, Jeremy R, Nima S Hejazi, Ivana Malenica, and Oleg Sofrygin. 2020. <em><code>sl3</code>: Modern Pipelines for Machine Learning and Super Learning</em>. <a href="https://github.com/tlverse/sl3">https://github.com/tlverse/sl3</a>. <a href="https://doi.org/10.5281/zenodo.1342293">https://doi.org/10.5281/zenodo.1342293</a>.</p>
</div>
<div id="ref-diaz2020causal">
<p>Dı́az, Iván, and Nima S Hejazi. 2020. “Causal Mediation Analysis for Stochastic Interventions.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 82 (3): 661–83. <a href="https://doi.org/10.1111/rssb.12362">https://doi.org/10.1111/rssb.12362</a>.</p>
</div>
<div id="ref-robins1992identifiability">
<p>Robins, James M, and Sander Greenland. 1992. “Identifiability and Exchangeability for Direct and Indirect Effects.” <em>Epidemiology</em>, 143–55.</p>
</div>
<div id="ref-tchetgen2012semiparametric">
<p>Tchetgen Tchetgen, Eric J, and Ilya Shpitser. 2012. “Semiparametric Theory for Causal Mediation Analysis: Efficiency Bounds, Multiple Robustness, and Sensitivity Analysis.” <em>Annals of Statistics</em> 40 (3): 1816–45. <a href="https://doi.org/10.1214/12-AOS990">https://doi.org/10.1214/12-AOS990</a>.</p>
</div>
<div id="ref-vanderweele2015explanation">
<p>VanderWeele, Tyler. 2015. <em>Explanation in Causal Inference: Methods for Mediation and Interaction</em>. Oxford University Press.</p>
</div>
<div id="ref-zheng2011cross">
<p>Zheng, Wenjing, and Mark J van der Laan. 2011. “Cross-Validated Targeted Minimum-Loss-Based Estimation.” In <em>Targeted Learning</em>, 459–74. Springer.</p>
</div>
<div id="ref-zheng2012targeted">
<p>———. 2012. “Targeted Maximum Likelihood Estimation of Natural Direct Effects.” <em>International Journal of Biostatistics</em> 8 (1). <a href="https://doi.org/10.2202/1557-4679.1361">https://doi.org/10.2202/1557-4679.1361</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Nima Hejazi, James Duncan, David McCoy.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>

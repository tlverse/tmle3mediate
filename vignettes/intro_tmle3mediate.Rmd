---
title: "Targeted Learning for Causal Mediation Analysis"
author: "[Nima Hejazi](https://nimahejazi.org), [James
  Duncan](https://statistics.berkeley.edu/people/james-duncan), and
  [David McCoy](http://bbd.berkeley.edu/cohort-4-2019-2020.html)"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Targeted Learning for Causal Mediation Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Background

A treatment often affects an outcome indirectly, through a particular pathway,
by its effect on _intermediate variables_ (mediators). Causal mediation analysis
concerns the construction and evaluation of these _indirect effects_ and the
_direct effects_ that are complementary to them. Generally, the indirect effect
(IE) of a treatment on an outcome is the portion of the total effect that is
found to work through mediators, while the direct effect includes all other
components of the total effect (including both the effect of the treatment on
the outcome and the effect through all paths not explicitly involving the
mediators).  Identifying and quantifying the mechanisms underlying causal
effects is an increasingly popular endeavor in public health, medicine, and the
social sciences, as such mechanistic knowledge improves understanding of both
_why_ and _how_ treatments may be effective.

While the study of mediation analysis may be traced back quite far, the field
only came into its modern form with the identification and careful study of the
natural direct and indirect effects [@robins1992identifiability;
pearl2001direct]. The natural direct effect (NDE) and the natural indirect
effect (NIE) are based on a decomposition of the average treatment effect (ATE)
in the presence of mediators [@vanderweele2015explanation], with requisite
theory for the construction of efficient estimators of these quantities only
receiving attention recently [@tchetgen2012semiparametric]. Here, we examine
the use of the `tmle3mediate` package for constructing targeted maximum
likelihood (TML) estimators of the NDE and NIE [@zheng2012targeted].

To make our methodology and results comparable to that exposed in existing
tools, we'll take as a running example a simple dataset from an observational
study of the relationship between BMI and kids' behavior, distributed as part of
the [`mma` R package on CRAN](https://CRAN.R-project.org/package=mma).  First,
let's load the packages we'll be using and set a seed; then, load this dataset
and take a quick look at it

```{r load_data, message=FALSE, warning=FALSE}
# preliminaries
library(dplyr)
library(tidyr)
library(sl3)
library(tmle3)
library(tmle3mediate)

# load and examine data
library(mma)
data(weight_behavior)
dim(weight_behavior)

# set a seed
set.seed(429153)
```

The documentation for the dataset describes it as a "database obtained from the
Louisiana State University Health Sciences Center, New Orleans, by  Dr. Richard
Scribner. He  explored the relationship  between BMI and kids behavior through a
survey at children, teachers and parents in Grenada in 2014. This data set
includes 691 observations and 15 variables."

Unfortunately, the dataset contains a few observations with missing values. As
these are unrelated to the demonstration of our analytic methodology, we'll
simply remove these for the time being. Note that in a real-world data analysis,
we would instead consider strategies for working with the observed data and
missing observations, including imputation and inverse probability of censoring
weighting. For now, we simply remove the incomplete observations, resulting in a
dataset with fewer observations but much the same structure as the original:

```{r remove_na, echo=FALSE, message=FALSE, warning=FALSE}
# remove missing values
weight_behavior_complete <- weight_behavior %>%
  drop_na() %>%
  mutate(
    sports = as.numeric(sports) - 1
  ) %>%
  as_tibble()
head(weight_behavior_complete)
```

For the analysis of this observational dataset, we focus on the effect of
participating in a sports team (`sports`) on the BMI of children (`bmi`), taking
several related covariates as mediators (`snack`, `exercises`, `overweigh`) and
all other collected covariates as potential confounders. Considering an NPSEM,
we separate the observed variables from the dataset into their corresponding
nodes as follows:

```{r npsem, message=FALSE, warning=FALSE}
exposure <- "sports"
outcome <- "bmi"
mediators <- c("snack", "exercises", "overweigh")
covars <- setdiff(colnames(weight_behavior_complete),
                  c(exposure, outcome, mediators))
node_list <- list(
  W = covars,
  A = exposure,
  Z = mediators,
  Y = outcome
)
```

Here the `node_list` encodes the parents of each node -- for example, $Z$ (the
mediators) has parents $A$ (the treatment) and $W$ (the baseline confounders),
and $Y$ (the outcome) has parents $Z$, $A$, and $W$.

To use the natural (in)direct effects for mediation analysis, we decompose the
ATE into its corresponding direct and indirect effect components. Throughout,
we'll make use of the (semiparametric efficient) TML estimators of
@zheng2012targeted, which allow for flexible ensemble machine learning to be
incorporated into the estimation of nuisance parameters. For this, we rely on
the [`sl3` R package](https://tlverse.org/sl3) [@coyle2020sl3], which provides
an implementation of machine learning pipelines and the Super Learner algorithm;
for more on the `sl3` R package, consider consulting [this chapter of the
`tlverse`
handbook](https://tlverse.org/tlverse-handbook/ensemble-machine-learning.html).
Below, we construct an ensemble learner using a handful of popular machine
learning algorithms:

```{r make_sl_learners, message=FALSE, warning=FALSE}
# SL learners used for continuous data (the nuisance parameter M)
hal_contin_learner <- Lrnr_hal9001$new(
  max_degree = 3, fit_type = "glmnet", n_folds = 3
)
enet_contin_learner <- Lrnr_glmnet$new(
  alpha = 0.5, family = "gaussian", nfolds = 3
)
lasso_contin_learner <- Lrnr_glmnet$new(
  alpha = 1, family = "gaussian", nfolds = 3
)
fglm_contin_learner <- Lrnr_glm_fast$new(family = gaussian())
contin_learner_lib <- Stack$new(
  hal_contin_learner, enet_contin_learner, lasso_contin_learner,
  fglm_contin_learner
)
sl_contin_learner <- Lrnr_sl$new(learners = contin_learner_lib,
                                 metalearner = Lrnr_nnls$new())

# SL learners used for binary data (nuisance parameters G and E in this case)
hal_binary_learner <- Lrnr_hal9001$new(
  max_degree = 3, fit_type = "glmnet", n_folds = 3, family = "binomial"
)
enet_binary_learner <- Lrnr_glmnet$new(
  alpha = 0.5, family = "binomial", nfolds = 3
)
lasso_binary_learner <- Lrnr_glmnet$new(
  alpha = 1, family = "binomial", nfolds = 3
)
fglm_binary_learner <- Lrnr_glm_fast$new(family = binomial())
binary_learner_lib <- Stack$new(
  hal_binary_learner, enet_binary_learner, lasso_binary_learner,
  fglm_binary_learner
)
logistic_metalearner <- make_learner(Lrnr_solnp,
                                     metalearner_logistic_binomial,
                                     loss_loglik_binomial)
sl_binary_learner <- Lrnr_sl$new(learners = binary_learner_lib,
                                 metalearner = logistic_metalearner)

# create list for treatment and outcome mechanism regressions
learner_list <- list(
  Y = sl_contin_learner,
  A = sl_binary_learner
)
```

## Decomposing the Average Treatment Effect

The natural direct and indirect effects arise from a decomposition of the ATE:
\begin{equation*}
  \mathbb{E}[Y(1) - Y(0)] =
    \underbrace{\mathbb{E}[Y(1, Z(0)) - Y(0, Z(0))]}_{NDE} +
    \underbrace{\mathbb{E}[Y(1, Z(1)) - Y(1, Z(0))]}_{NIE}.
\end{equation*}

Consider the joint likelihood:
\begin{equation*}
  p(w, a, z, y) = \underbrace{p(y \mid w, a, z)}_{Q_Y(A, W, Z)}
  \underbrace{p(z \mid w, a)}_{Q_Z(Z \mid A, W)}
  \underbrace{p(a \mid w)}_{g(A \mid W)}
  \underbrace{p(w)}_{Q_W}
\end{equation*}

\textbf{The NDE is defined as:}
\begin{equation*}
  \Psi_{NDE}=\mathbb{E}[Y(1, Z(0)) - Y(0, Z(0))]
  \overset{\text{rand.}}{=} \sum_w \sum_z
  [\underbrace{\mathbb{E}(Y \mid A = 1, w, z)}_{\bar{Q}_Y(A = 1, w, z)} -
  \underbrace{\mathbb{E}(Y \mid A = 0, w, z)}_{\bar{Q}_Y(A = 0, w, z)}] \times
  \underbrace{p(z \mid A = 0, w)}_{Q_Z(0, w))} \underbrace{p(w)}_{Q_W}
\end{equation*}

We begin estimating the NDE by using $\hat{\bar{Q}}_Y$, the estimator for
$Q_Y(A, W, Z)$ above ($Y$ given $Z$, $A$, and $W$) to obtain the conditional
expectations $\bar{Q}_Y(W, 1, Z)$ for $A = 1$ and likewise $\bar{Q}_Y(W,0,Z)$
for $A = 0$. We determine this difference $\bar{Q}_Y(W,1,Z)$ -
$\bar{Q}_Y(W,0,Z)$, we call this $Q_{diff}$. $Q_{diff}$ is then regressed on $W$
among control observations only (i.e., $A = 0$).  Overall, $Q_{diff}$ represents
the difference in $Y$ due to $A$ while keeping $Z$ and $W$ at their natural
values. When we regress this difference of $W$ among the controls we are getting
the expected $Q_{diff}$ if all individuals had $A = 0$ covariates. Since $W$ is
a parent of $Z$ by regressing this portion of $W$ on $Q_{diff}$ we can remove
part of the marginal impact of $Z$ on $Q_{diff}$. Any residual additive effect
of $Z$ on $Q_{diff}$ is removed during the TML estimation step using the
mediator auxiliary covariate.

The clever covariate for this estimate is defined as:
\begin{equation*}
  C_Y(Q_Z,g)(O) = \Bigg\{\frac{\mathbb{I}(A = 1)}{g(1 \mid w)}
  \frac{Q_Z(Z \mid W, 0)}{Q_Z(Z \mid W, 1)} -
  \frac{\mathbb{I}(A = 0)}{g(0 \mid W)} \Bigg\}
\end{equation*}

Breaking this down, $\frac{\mathbb{I}(A = 1)}{g(1 \mid w)}$ is the inverse
probability weight for $A = 1$ and, likewise, $\frac{\mathbb{I}(A = 0)}{g(0 \mid
w)}$ is the inverse probability weight for $A = 0$. The term in the middle is a
ratio of the mediator density when $A = 0$ over the mediator density when $A =
1$.

Therefore, it appears that we would need to estimate the density of our
mediatiors $Z$ which can be computationally taxing and statistically challenging
when $Z$ is high-dimensional. However, given that only the ratio of these
densities is required, a convenient re-parametrization may be achieved
\begin{equation*}
  \frac{p(A = 0 \mid Z, W) g(0 \mid W)}{ p(A = 1 \mid Z, W) g(1 \mid W)}
\end{equation*}
This is particularly useful since the problem is reduced to the estimation of
conditional means, allowing for the diversity of algorithms developed in machine
learning to be brought to bear on the problem.

Underneath the hood, the difference of the nuisance estimates of the outcome
regression under the treatment ($A = 1$) and control ($A = 0$) conditions is
used to construct $Q_{diff}$
as determined by $W$ when $A = 0$ and our $e$
estimation which is the probability of $A$ given $Z$ and $W$ which we use to
construct the clever coefficient above. We call these nuisance parameters and
once determined we then pass them to the TMLE step for final estimation.

\textbf{The NIE is defined as:}
Derivation and estimation of the natural indirect effect is analogous to the
direct effect. Again, this is the effect of $A$ on $Y$ through a mediator $Z$.
This quantity is known as the additive natural indirect effect $E(Y(1,Z(1)) -
E(Y(1,Z(0))$ which means the expectation of $Y$ given $A = 1$ and $Z$ values
when $A = 1$, minus the expectation of $Y$ given $A = 1$ and $Z$ values when
$A = 0$.

As with the NDE, the re-parameterization trick allows us to estimate $E(A \mid
Z, W)$ rather than estimating a multivariate conditional density. However now,
our $\Psi_Z(Q)$ or mediated mean outcome difference is estimated as:
\begin{equation*}
  \Psi_{NIE}(Q) = E_{QZ}(\psi_{NIE,Z}(Q)(W,1) - \psi_{NIE,Z}(Q)(W,0))
\end{equation*}

In plain language, this uses the $\bar{Q}_Y(W,1, Z)$ or the predicted values for
$Y$ given $W$ and $Z$ when $A = 1$ and "regresses" this vector on $W$ among the
treated to obtain the conditional mean $\Psi_{NIE,Z}(Q)(W,1)$. The same
procedure is done, regressing $Y$ given $W$ and $Z$ when $A = 1$ on $W$ among
the control observations, to obtain $\Psi_{NIE,Z}(Q)(W,0)$. The difference of
these two estimates is the NIE and can be thought of as the additive marginal
effect of treatment on the conditional expectation of $Y$ given $W$, $A = 1$,
$Z$ through its effects on $Z$. So in the case of the NIE our $\Psi$ estimate is
slightly different but the $e$ used for the clever covariate is the same.

## Targeted Maximum Likelihood

With each nuisance parameter estimated using flexible machine learning
algorithms we can use targeted maximum likelihood to obtain a targeted fit for
both our NIE and NDE estimates.

With targeted maximum likelihood estimation (TMLE), the portion of $P_0$ that
the target parameter is a function of is estimated, this is done by utilizing
additional components of $P_0$ outside of only $Q_y$ - which, as seen above is
the only component to be used so far (for our $\Psi$ estimates). Because $Q_y$
is a more complicated estimate the variance and bias is 'spread' across our
parameter space, TMLE is an additional step to help target our parameter of
interest and reduce bias/variance for this parameter. The steps to TMLE for a
standard average treatment effect are:

1. Estimate $Q_y$ using flexible machine learning,
2. Get predictions from the model for A = a and W = w, this is the initial
   prediction values for Y $\hat{Q}^0_n(A,W)$,
3. We then use clever covariates (like the ones described above) in a parametric
   sub-model through the initial fit of $\hat{Q}^0_n(A,W)$, and estimate the
   unknown parameter of this submodel (the coefficients) which represents the
   amount of fluctuation of the initial fit (amount of residual confounding).

This is done by constructing a logistic regression flucuation model which has
the true Y as outcome, offset by the initial estimates $\hat{Q}^0_n(A,W)$ and
the clever coefficient as independent variable. Once fit, the coefficient is
used to update the initial predictions $\hat{Q}^0_n(A,W)$ to make
$\hat{Q}^1_n(A,W)$, if convergence to zero is not attained then this estimate
$\hat{Q}^1_n(A,W)$ can be used in the next iteration of the logistic regression
as an offset with Y as outcome and the clever coefficient to attain
$\hat{Q}^2_n(A,W)$. Usually, convergence is made in one step however. At each
step, the initial counterfactual estimates from the model are updated by:
$$logit\bar{Q}^1_{n}(A,W) = logit\bar{Q}^0_n(A,W) +
  \epsilon_n H_n^{\star}(A,W)$$
$$logit\bar{Q}^1_{n}(1,W) = logit\bar{Q}^0_n(1,W) + \epsilon_n
  H_n^{\star}(1,W)$$
$$logit\bar{Q}^1_{n}(0,W) = logit\bar{Q}^0_n(0,W) + \epsilon_n
  H_n^{\star}(0,W)$$,
for all observations

Until convergence ($\epsilon \sim 0$). Here, $\bar{Q}^0_n(1,W)$ and
$\bar{Q}^0_n(0,W)$ are the initial counterfactual Y outcomes before updating,
$H_n^{\star}(1,W)$ is the clever covariate for treated observations ($A=1$) and
$H_n^{\star}(0,W)$ is the clever covariate for control cases ($A=0$) and
$\epsilon$ is the coefficient from the logistic regression model.

For the NIE and NDE, the same procedure is conducted however our updated
estimates, $logit\bar{Q}^{\star}_{n}(A,W)$, $logit\bar{Q}^{\star}_{n}(1,W)$, and
$logit\bar{Q}^{\star}_{n}(0,W)$ backpropogate to our $\Psi_{NDE}$ and
$\Psi_{NIE}$ estimates. So, for example, after our first TMLE fit, the
difference between $logit\bar{Q}^1_{n}(1,W)$ and $logit\bar{Q}^1_{n}(0,W)$ now
become $Q_{diff}$ which is then regressed on W where $A = 0$ to obtain our
updated $\Psi_{NDE}$.  Likewise, $logit\bar{Q}^1_{n}(1,W)$ is passed back to our
$\Psi_{NIE}$ and used as the outcome when regressing on W when $A = 1$, and $W$
when $A = 0$, as described above. Note, here we use the term regressing to mean
estimating using flexible machine learning algorithms.

## Estimating the Natural Indirect Effect

Now we can calculate the NIE by creating a spec object which gives instructions
on which learners to use for our nuisance parameters $e$ and $\psi_z$. We then
provide `tmle3` with our Spec object, the data, the node list we created above,
and a learner list that tells the software which learners to use for estimating
$A$ and $Y$.

```{r NIE, message=FALSE, warning=FALSE}
tmle_spec_NIE <- tmle_NIE(
  e_learners = Lrnr_cv$new(hal_binary_learner, full_fit = TRUE),
  psi_Z_learners = Lrnr_cv$new(hal_contin_learner, full_fit = TRUE),
  max_iter = 10
)
weight_behavior_NIE <- tmle3(
  tmle_spec_NIE, weight_behavior_complete, node_list, learner_list
)
print(weight_behavior_NIE)
```

We can extract our TML estimate for the NIE by
```{r tmle_est}
weight_behavior_NIE$summary$tmle_est
```

## Estimating the Natural Direct Effect

The same procedure is done to get the natural direct effect but we use the
`tmle_spec_NDE` to define the learners that will measure the NDE parameters:

```{r NDE, message=FALSE, warning=FALSE}
tmle_spec_NDE <- tmle_NDE(
  e_learners = Lrnr_cv$new(hal_binary_learner, full_fit = TRUE),
  psi_Z_learners = Lrnr_cv$new(hal_contin_learner, full_fit = TRUE),
  max_iter = 10
)
weight_behavior_NDE <- tmle3(
  tmle_spec_NDE, weight_behavior_complete, node_list, learner_list
)
weight_behavior_NDE <- weight_behavior_NDE$summary
print(weight_behavior_NDE)
```

<!--
Let's check out what MMA looks like for this sports behavior -> BMI dataset as
mediated by these same three covariates:

```{r mma_weight_mediation, message=FALSE, warning=FALSE}
x <- weight_behavior_complete[, c(2:11, 13:15)]
pred <- weight_behavior_complete[, 12]
y <- data.frame(weight_behavior_complete[, 1])
colnames(y) <- "bmi"

mma_glm_weight <- mma(x, y, pred = pred, mediator = c(7, 11, 13),
                      jointm = list(n = 1, j1 = c(7, 11, 13)), predref = 0,
                      alpha = 0.4, alpha2 = 0.4, n2 = 20, nonlinear = FALSE)
print(mma_glm_weight)

mma_mart_weight <- mma(x, y, pred = pred, mediator = c(7, 11, 13),
                       jointm = list(n = 1, j1 = c(7, 11, 13)), predref = 0,
                       alpha = 0.4, alpha2 = 0.4, n = 2, n2 = 1,
                       nonlinear = TRUE)
print(mma_mart_weight)
```
-->

<!--
Now let's generate some simulated data where we know the truth and compare our
results using `tmle3mediate` to the `mma` package where we got the previous
data from:
```{r gen_data}
data_sim <- sim_data(n_obs = 2000)
head(data_sim)
```

```{r fit_tmle3mediate_sim}
node_list <- list(
  W = c("W1", "W2", "W3"),
  A = "A",
  Z = c("Z1", "Z2", "Z3"),
  Y = "Y"
)

learner_list <- list(
  Y = cv_hal_contin_lrnr,
  A = cv_hal_binary_lrnr
)
tmle_spec_NIE <- tmle_NIE(
  e_learners = cv_hal_binary_lrnr,
  psi_Z_learners = cv_hal_contin_lrnr,
  max_iter = 10
)
NIE_est <- tmle3(tmle_spec_NIE, data_sim, node_list, learner_list)

tmle_spec_NDE <- tmle_NDE(
  e_learners = cv_hal_binary_lrnr,
  psi_Z_learners = cv_hal_contin_lrnr,
  max_iter = 10
)
NDE_est <- tmle3(tmle_spec_NDE, data_sim, node_list, learner_list)
```

Let's use mma to find the NDE and NIE on the same simulated data:
```{r mma, message=FALSE, warning=FALSE}
x <- data.frame(data_sim[, c(1:3, 5:7)])
pred <- data.frame(data_sim[, 4])
y <- data.frame(data_sim[, 8])
colnames(y) <- "Y"

mma_glm <- mma(x, y, pred = pred, mediator = c(4:6),
               jointm = list(n = 1, j1 = c(4:6)), predref = 0,
               alpha = 0.4, alpha2 = 0.4, n2 = 20, nonlinear = FALSE)
print(mma_glm)

mma_mart <- mma(x, y, pred = pred, mediator = c(4:6),
                jointm = list(n = 1, j1 = c(4:6)), predref = 0,
                alpha = 0.4, alpha2 = 0.4, n2 = 1, nonlinear = TRUE)
print(mma_glm)
```
-->

# The Population Intervention (In)Direct Effects

## Background

We are interested in assessing the population intervention direct effect and
the population intervention indirect effect, based on the effect decomposition
of the population intervention effect introduced in @diaz2020causal.

Finally, in our analysis, we consider an incremental propensity score
intervention (IPSI), as first proposed by @kennedy2017nonparametric, wherein the
_odds of participating in a sports team_ is modulated by some fixed amount
($0 \leq \delta \leq \infty$) for each individual. Such an intervention may be
interpreted as the effect of a school program that motivates children to
participate in sports teams. To exemplify our approach, we postulate a
motivational intervention that _triples the odds_ of participating in a sports
team for each individual:

```{r delta_ipsi, message=FALSE, warning=FALSE}
delta_shift_ipsi <- 2
```

## Decomposing the Population Intervention Effect

We may decompose the population intervention effect (PIE) in terms of a
_population intervention direct effect_ (PIDE) and a _population
intervention indirect effect_ (PIIE):
\begin{equation*}
  \overbrace{\mathbb{E}\{Y(A_\delta, Z(A_\delta)) -
    Y(A_\delta, Z)\}}^{\text{PIIE}} +
    \overbrace{\mathbb{E}\{Y(A_\delta, Z) - Y(A, Z)\}}^{\text{PIDE}}.
\end{equation*}

This decomposition of the PIE as the sum of the population intervention direct
and indirect effects has an interpretation analogous to the corresponding
standard decomposition of the average treatment effect. In the sequel, we will
compute each of the components of the direct and indirect effects above using
appropriate estimators as follows

* For $\mathbb{E}\{Y(A, Z)\}$, the sample mean $\frac{1}{n}\sum_{i=1}^n Y_i$ is
  sufficient;
* for $\mathbb{E}\{Y(A_{\delta}, Z)\}$, an efficient one-step estimator for the
  effect of a joint intervention altering the exposure mechanism but not the
  mediation mechanism, as proposed in @diaz2020causal; and,
* for $\mathbb{E}\{Y(A_{\delta}, Z_{A_{\delta}})\}$, an efficient one-step
  estimator for the effect of a joint intervention altering both the exposure
  and mediation mechanisms, as proposed in @kennedy2017nonparametric and
  implemented in the [`npcausal` R
  package](https://github.com/ehkennedy/npcausal).

## Estimating the Effect Decomposition Term

As given in @diaz2020causal, the statistical functional identifying the
decomposition term that appears in both the PIDE and PIIE
$\mathbb{E}\{Y(A_{\delta}, Z)\}$, which corresponds to altering the exposure
mechanism while keeping the mediation mechanism fixed, is
\begin{equation*}
  \theta_0(\delta) = \int m_0(a, z, w) g_{0,\delta}(a \mid w) p_0(z, w)
    d\nu(a, z, w),
\end{equation*}
for which a one-step estimator is available. The corresponding _efficient
influence function_ (EIF) with respect to the nonparametric model $\mathcal{M}$
is $D_{\eta,\delta}(o) = D^Y_{\eta,\delta}(o)
+ D^A_{\eta,\delta}(o) + D^{Z,W}_{\eta,\delta}(o) - \theta(\delta)$. The
one-step estimator may be computed using the EIF estimating equation, making use
of cross-fitting [@zheng2011cross; @chernozhukov2018double] to circumvent any
need for entropy conditions (i.e., Donsker class restrictions). The resultant
estimator is
\begin{equation*}
  \hat{\theta}(\delta) = \frac{1}{n} \sum_{i = 1}^n D_{\hat{\eta}_{j(i)},
  \delta}(O_i) = \frac{1}{n} \sum_{i = 1}^n \left\{ D^Y_{\hat{\eta}_{j(i)},
  \delta}(O_i) + D^A_{\hat{\eta}_{j(i)}, \delta}(O_i) +
  D^{Z,W}_{\hat{\eta}_{j(i)}, \delta}(O_i) \right\},
\end{equation*}
which is implemented in the `medshift` R package. We make use of that
implementation to estimate $\mathbb{E}\{Y(A_{\delta}, Z)\}$ via its one-step
estimator $\hat{\theta}(\delta)$ below

```{r efficient_est, message=FALSE, warning=FALSE}
# instantiate tmle3 spec for stochastic mediation
tmle_spec_medshift <- tmle_medshift(
  delta = delta_ipsi,
  e_learners = Lrnr_cv$new(hal_binary_learner, full_fit = TRUE),
  phi_learners = Lrnr_cv$new(hal_contin_learner, full_fit = TRUE)
)

# compute the TML estimate
weight_behavior_medshift <- tmle3(
  tmle_spec_medshift, weight_behavior_complete, node_list, learner_list
)
print(weight_behavior_medshift)
```

## Estimating the Direct Effect

Recall that, based on the decomposition outlined previously, the population
intervention direct effect may be denoted $\beta_{\text{PIDE}}(\delta) =
\theta_0(\delta) - \mathbb{E}Y$. Thus, an estimator of the PIDE,
$\hat{\beta}_{\text{PIDE}}(\delta)$ may be expressed as a composition of
estimators of its constituent parameters:
\begin{equation*}
  \hat{\beta}_{\text{PIDE}}({\delta}) = \hat{\theta}(\delta) -
  \frac{1}{n} \sum_{i = 1}^n Y_i.
\end{equation*}

Based on the above, we may construct an estimator of the PIDE using quantities
already computed. The convenience function below applies the simple delta method
required in the case of a linear contrast between the two constituent
parameters:
```{r linear_contrast_delta, message=FALSE, warning=FALSE, eval=FALSE}
# convenience function to compute inference via delta method: EY1 - EY0
linear_contrast <- function(params, eifs, ci_level = 0.95) {
  # bounds for confidence interval
  ci_norm_bounds <- c(-1, 1) * abs(stats::qnorm(p = (1 - ci_level) / 2))
  param_est <- params[[1]] - params[[2]]
  eif <- eifs[[1]] - eifs[[2]]
  se_eif <- sqrt(var(eif) / length(eif))
  param_ci <- param_est + ci_norm_bounds * se_eif
  # parameter and inference
  out <- c(param_ci[1], param_est, param_ci[2])
  names(out) <- c("lwr_ci", "param_est", "upr_ci")
  return(out)
}
```

With the above convenience function in hand, we'll construct or extract the
necessary components from existing objects and simply apply the function:
```{r comp_de_binary, message=FALSE, warning=FALSE, eval=FALSE}
# parameter estimates and EIFs for components of direct effect
EY <- mean(Y)
eif_EY <- Y - EY
params_de <- list(theta_eff$theta, EY)
eifs_de <- list(theta_eff$eif, eif_EY)

# direct effect = EY - estimated quantity
de_est <- linear_contrast(params_de, eifs_de)
de_est
```

# References

